{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac02fc29",
   "metadata": {},
   "source": [
    "# Task 3: Object detection and Tracking\n",
    "For this task you will \n",
    "1. Use YOLO to detect people\n",
    "2. Estimate the distance to the detected persons\n",
    "3. Track the detected persons using a Kalman Filter\n",
    "\n",
    "## Important hints to successfully complete this task\n",
    "- **Donâ€˜t** change blocks except you are explicitly asked to do so\n",
    "- Add your solution in the #TODO parts\n",
    "- To pass this task, **every cell** of the notebook needs to be executable i.e. runs without errors\n",
    "- The notebooks are manually checked, if the task was understood and solved in a reasonable way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa545f4",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "\n",
    "If you are missing modules install them to you selected kernel via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f808f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # Import cv2 library for image processing\n",
    "import pyrealsense2 as rs # Import realsense python library\n",
    "import numpy as np \n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f7dd6",
   "metadata": {},
   "source": [
    "## 3.1 Start the RealSense and view the output\n",
    "\n",
    "Attach a realsense camera to the pc on which you are executing this notebook and check if the stream works by executing the cell underneath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b0525c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m config\u001b[38;5;241m.\u001b[39menable_stream(rs\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mdepth, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m, rs\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mz16, \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Start the pipeline with the configuration\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Enable interactive mode in matplotlib for real-time updating\u001b[39;00m\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mion()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "# Create a new pipeline object for streaming data\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a configuration object to configure the streams\n",
    "config = rs.config()\n",
    "\n",
    "# Enable color stream with a resolution of 640x480, BGR8 format, and a frame rate of 30 FPS\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Enable depth stream with a resolution of 640x480, Z16 format, and a frame rate of 30 FPS\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "# Start the pipeline with the configuration\n",
    "pipeline.start(config)\n",
    "\n",
    "# Enable interactive mode in matplotlib for real-time updating\n",
    "plt.ion()\n",
    "\n",
    "# Start an infinite loop to continuously fetch frames and display the color stream\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a new set of frames (color and depth)\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Extract the color frame from the frames object\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Convert the color frame to a numpy array for visualization\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Convert BGR to RGB for correct color representation in matplotlib\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Clear the previous output in the interactive mode to update the plot\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Display the color image using matplotlib\n",
    "        plt.imshow(color_image)\n",
    "        plt.axis('off')  # Turn off the axis labels for a clean view\n",
    "        plt.show()\n",
    "\n",
    "        # Sleep for 0.01 seconds to control the frame rate (approximately 100 FPS)\n",
    "        time.sleep(0.01)  # Adjust sleep time to control the FPS\n",
    "\n",
    "# Handle keyboard interrupt gracefully (e.g., pressing Ctrl+C to stop)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Stop the pipeline after the loop ends\n",
    "pipeline.stop()\n",
    "\n",
    "# Print a message indicating that streaming has been stopped\n",
    "print(\"Streaming stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d879534",
   "metadata": {},
   "source": [
    "# 3.2 Load the YOLO model and detect object with the RealSense\n",
    "\n",
    "In addition to simply watching the video stream of the realsense camera load a pretrained yolo model. Use the model to predict bounding boxes for people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0727cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(estimate_depth=False, kalman_tracking=False):\n",
    "\n",
    "    ### TODO: load the yolo model, hint look into ultralytics documentation\n",
    "    model = YOLO(\"yolo11n.pt\")\n",
    "    ###\n",
    "\n",
    "    # realsense config\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    pipeline.start(config)\n",
    "    plt.ion()\n",
    "\n",
    "    # create var to store last bounding box\n",
    "    last_bbox = (0,0,0,0)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            # receive color and depth frames\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "            ### TODO: run yolo model to predict a bounding boxes only for people, select only the best detection\n",
    "            results = model(color_image)\n",
    "            ###\n",
    "\n",
    "            result_img = results.plot()\n",
    "\n",
    "            # create var to store detected positions\n",
    "            detected_position = None\n",
    "\n",
    "            # process the results\n",
    "            for box in results.boxes:\n",
    "                # get bounding box coordinates and store them\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0]) \n",
    "                \n",
    "                ### TODO: calculate the center of the bounding boxes and store the center detections in the tuple detected_positions like (x_value, y_value)\n",
    "                center_x = (x1 + x2)// 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                detected_position = (center_x, center_y)\n",
    "                ###\n",
    "\n",
    "                # store detections\n",
    "                last_bbox = (x1, y1, x2, y2)\n",
    "\n",
    "                # check if bbox is for a person and  if depth should be estimated\n",
    "                if int(box.cls) == 0 and estimate_depth:\n",
    "                    # estimate distances\n",
    "                    person_distance_center = dist_to_center(depth_image, center_x, center_y)\n",
    "                    person_distance_mean, person_distance_median = dist_to_cutout(depth_image, x1, x2, y1, y2)\n",
    "\n",
    "                    # create distance labels and put on image\n",
    "                    label_distance_center = f'distance center: {person_distance_center}'  # model class names list for labels\n",
    "                    label_distance_mean = f'distance mean: {person_distance_mean}'  # model class names list for labels\n",
    "                    label_distance_median = f'distance median: {person_distance_median}'  # model class names list for labels\n",
    "                    cv2.putText(result_img, label_distance_center, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    cv2.putText(result_img, label_distance_mean, (x1, y1 + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    cv2.putText(result_img, label_distance_median, (x1, y1 + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # check if kalman filter tracking should be activated\n",
    "            if kalman_tracking:\n",
    "                # track with kalman filter\n",
    "                pred_x_min, pred_y_min, pred_x_max, pred_y_max = kalman_filter_tracking(detected_position, last_bbox)\n",
    "                # draw the predicted bounding box (blue)\n",
    "                cv2.rectangle(result_img, (pred_x_min, pred_y_min), (pred_x_max, pred_y_max), (0, 0, 255), 2)\n",
    "        \n",
    "            # plot image\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(result_img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            # update frequency\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    pipeline.stop()\n",
    "    print(\"Streaming stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146ad7d",
   "metadata": {},
   "source": [
    "Check if the tracking works by executing the main loop. But set `estimate_depth` parameter to false as we did not define the needed functions for this functionality until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50085239",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop(estimate_depth=False, kalman_tracking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea921cf9",
   "metadata": {},
   "source": [
    "# 3.3 Calculate the distance to a person\n",
    "For calculating the distance you can use the depth information of the realsense camera. There are different ways to calculate the distance to an object:\n",
    "1. Distance to the center of a bounding box\n",
    "2. Median or mean distance to the cutout of the object\n",
    "\n",
    "Implement the above mentioned metrics in the two following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_to_center(depth_image, x_center, y_center):\n",
    "    \n",
    "    ### TODO: estimate the distance to the person based on the given center and return the distance\n",
    "    return depth_image[y_center, x_center]\n",
    "    ###\n",
    "\n",
    "def dist_to_cutout(depth_image, x1, x2, y1, y2):\n",
    "    \n",
    "    ### TODO: estimate the center of the given bounding box values and return their mean and median in this order\n",
    "    distance = [depth_image[y , x] for y in range (y1,y2) for x in range (x1,x2)]\n",
    "    return np.mean(distance), np.median(distance)\n",
    "    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop(estimate_depth=True, kalman_tracking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaee12c",
   "metadata": {},
   "source": [
    "# 3.4 Kalman Filter\n",
    "Classical tracking with YOLO is not capable of handling occlusions. Therefore we use kalman filter tracking for predicting movements of occluded people. First we define the parameters of the kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kalman filter for 2D tracking (position and velocity in X and Y)\n",
    "kalman = cv2.KalmanFilter(4, 2)\n",
    "\n",
    "### TODO: define the position measurement matrix as a numpy array\n",
    "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]],np.float32)\n",
    "###\n",
    "\n",
    "# Transition matrix (state transition: position and velocity)\n",
    "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\n",
    "\n",
    "### TODO: define process noise covariance (controls how much the prediction \"trusts\" previous values) and measurement noise covariance (controls how much the measurement is trusted) as numpy arrays\n",
    "kalman.processNoiseCov = np.array([[0.01, 0, 0, 0], [0, 0.01, 0, 0], [0, 0, 0.01, 0], [0, 0, 0, 0.01]], np.float32)\n",
    "kalman.measurementNoiseCov = np.array([[1, 0], [0, 1]],np.float32)* 0.01\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c9c9e",
   "metadata": {},
   "source": [
    "Next we define the tracking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0815dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter_tracking(detected_position, bbox):\n",
    "\n",
    "    # predict the next position using kalman filter\n",
    "    predicted = kalman.predict()\n",
    "    predicted_x, predicted_y = int(predicted[0]), int(predicted[1])\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # if the person is detected, correct the kalman filter\n",
    "    if detected_position:\n",
    "        center_x, center_y = detected_position\n",
    "        # correct the kalman filter with the actual position (center of the bounding box)\n",
    "        kalman.correct(np.array([[np.float32(center_x)], [np.float32(center_y)]]))\n",
    "\n",
    "    ### TODO: draw the predicted bounding box based on the predicted center\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    pred_x_min = int(predicted_x - width / 2)\n",
    "    pred_y_min = int(predicted_y - height / 2)\n",
    "    pred_x_max = int(predicted_x + width / 2)\n",
    "    pred_y_max = int(predicted_y + height / 2)\n",
    "    ###\n",
    "\n",
    "    return pred_x_min, pred_y_min, pred_x_max, pred_y_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d4bcb",
   "metadata": {},
   "source": [
    "Now we can execute the main loop again with activated kalman filter tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979dc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop(estimate_depth=False, kalman_tracking=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
